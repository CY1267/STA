{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Homework 8: (a) Posterior Predictive Distributions\n",
        " 1. Describe how the posterior predictive distribution is created for mixture models"
      ],
      "metadata": {
        "id": "cScw-coNXJra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mixture Model predicts a new set of data points from the observed data and the prior distribution, thereby obtaining the posterior predictive distribution. First, we define the mixture model with parameters of specific components and mixture weights. Specific components refer to Gaussian distribution and so on, while parameters include key elements such as mean and variance. Mixture weights refer to the probability of each component. Second, Bayesian inference is used to update the weights and beliefs of the component parameters of the mixture model based on the observed data. In other words, the posterior distribution, prior distribution, and mixing proportions of the model parameters are estimated. Third, an efficient sampling method (such as MCMC) should be chosen to sample from the posterior distribution of the mixture model parameters. Finally, a component is randomly selected based on the sampled mixture weights to predict new data points of each sampled parameter set. After repeating the above steps, a collection of data points will be obtained, which forms the posterior prediction distribution of the model.\n"
      ],
      "metadata": {
        "id": "bfedImswXXRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. Describe how the posterior predictive distribution is created in general"
      ],
      "metadata": {
        "id": "iFgdncoh3A_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before calculating the posterior distribution, we need to establish a Bayesian model with the likelihood function and parameter prior distribution. Then, we use Bayesian inference to update the beliefs about the parameters based on the observed data, thereby estimating the posterior distribution of the parameters. Next, we sample from the posterior distribution of the model parameters and predict new data points from the likelihood function.  These new collections of data points form the posterior predictive distribution."
      ],
      "metadata": {
        "id": "iDPmx5NY387C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 8: (b) Missing Data Imputation\n",
        "3. Have glance through this and then describe how, if you were doing a regression of on but had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in X\n",
        "*   Hint: latent variables indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior\n",
        "analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...\n"
      ],
      "metadata": {
        "id": "Qr8-OWw93KKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If xx has missing values and we do not plan to discard the rows with missing values. We first need to define a Bayesian regression model and specify the priors and likelihood functions for all parameters related to xx and yy. Secondly, assign a prior to the missing value and update the belief through posterior analysis. The missing value is treated as an additional parameter by introducing a latent variable (vv) to be estimated. Thirdly, we can use the MCMC method to sample the posterior distribution of both model parameters and missing values. Fourthly, it is important to ensure that the MCAR assumptions fit the data since failure to hold the assumptions will cause bias in the estimates. Therefore, we need to verify that the probability of data loss is independent of the data itself, otherwise we need to consider more complex models. Finally, we will get the posterior distribution of the missing data after many iterations. This method incorporates the uncertainty of missing values into the analysis, which can reduce the estimation bias, leading to more reliable resultsã€‚"
      ],
      "metadata": {
        "id": "qnSPid2R83Tw"
      }
    }
  ]
}
